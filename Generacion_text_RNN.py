# -*- coding: utf-8 -*-
"""Generacion_poema_Embeddings_token.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VpSTgGAHIe3v2PGEhrL1gpsereHCbjkk
"""

import numpy as np
import io
import os

from google.colab import files
uploaded = files.upload()

file_name = "Ruben_Dario_poemas.txt"

# Abre y lee el contenido del archivo
with open(file_name, 'r', encoding='utf-8') as f:
    text = f.read()

# Imprime una porción del texto y la longitud total del texto
print(text[:600], len(text))

import string

all_characters = string.printable + "ñÑáÁéÉíÍóÓúÚ¿¡"
all_characters

import string

class Tokenizer():

  def __init__(self):
    self.all_characters = all_characters
    self.n_characters = len(self.all_characters)

  def text_to_seq(self, string):
    seq = []
    for c in range(len(string)):
        try:
            seq.append(self.all_characters.index(string[c]))
        except:
            continue
    return seq

  def seq_to_text(self, seq):
    text = ''
    for c in range(len(seq)):
        text += self.all_characters[seq[c]]
    return text

tokenizer = Tokenizer()
tokenizer.n_characters

tokenizer.text_to_seq('hola ¿como estas?')

tokenizer.seq_to_text([17, 24, 21, 10, 94, 112, 12, 24, 22, 24, 94, 14, 28, 29, 10, 28, 82])

"""Para entrenar nuestra red, vamos a necesitar secuencias de texto de una longitud determinada. Podemos generar estas ventanas con la siguiente función"""

text_encoded = tokenizer.text_to_seq(text)

train_size = len(text_encoded) * 80 // 100
train = text_encoded[:train_size]
test = text_encoded[train_size:]

len(train), len(test)

"""Para entrenar nuestra red, vamos a necesitar secuencias de texto de una longitud determinada. Podemos generar estas ventanas con la siguiente función"""

import random

def windows(text, window_size = 100):
    start_index = 0
    end_index = len(text) - window_size
    text_windows = []
    while start_index < end_index:
      text_windows.append(text[start_index:start_index+window_size+1])
      start_index += 1
    return text_windows

text_encoded_windows = windows(text_encoded)

print(tokenizer.seq_to_text((text_encoded_windows[0])))
print()
print(tokenizer.seq_to_text((text_encoded_windows[1])))
print()
print(tokenizer.seq_to_text((text_encoded_windows[2])))

"""Nuestro dataset de Pytorch se encargará de darnos cada una de estas frases, utilizando todos los carácteres excepto el último como entradas para la red y el último carácter como la etiqueta que usaremos durante el entrenamiento (la red deberá predecir la siguiente letra)."""

import torch

class CharRNNDataset(torch.utils.data.Dataset):
  def __init__(self, text_encoded_windows, train=True):
    self.text = text_encoded_windows
    self.train = train

  def __len__(self):
    return len(self.text)

  def __getitem__(self, ix):
    if self.train:
      return torch.tensor(self.text[ix][:-1]), torch.tensor(self.text[ix][-1])
    return torch.tensor(self.text[ix])

train_text_encoded_windows = windows(train)
test_text_encoded_windows = windows(test)

dataset = {
    'train': CharRNNDataset(train_text_encoded_windows),
    'val': CharRNNDataset(test_text_encoded_windows)
}

dataloader = {
    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=512, shuffle=True, pin_memory=True),
    'val': torch.utils.data.DataLoader(dataset['val'], batch_size=2048, shuffle=False, pin_memory=True),
}

len(dataset['train']), len(dataset['val'])

input, output = dataset['train'][0]
tokenizer.seq_to_text(input)

tokenizer.seq_to_text([output])

"""Embeddings

En Pytorch tenemos esta capa implementada en la clase **torch.nn.Embedding**.
"""

class CharRNN(torch.nn.Module):
  def __init__(self, input_size, embedding_size=128, hidden_size=256, num_layers=2, dropout=0.2):
    super().__init__()
    self.encoder = torch.nn.Embedding(input_size, embedding_size)
    self.rnn = torch.nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)
    self.fc = torch.nn.Linear(hidden_size, input_size)

  def forward(self, x):
    x = self.encoder(x)
    x, h = self.rnn(x)
    y = self.fc(x[:,-1,:])
    return y

model = CharRNN(input_size=tokenizer.n_characters)
outputs = model(torch.randint(0, tokenizer.n_characters, (64, 50)))
outputs.shape

from tqdm import tqdm
import numpy as np

device = "cuda" if torch.cuda.is_available() else "cpu"

def fit(model, dataloader, epochs=10):
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = torch.nn.CrossEntropyLoss()
    for epoch in range(1, epochs+1):
        model.train()
        train_loss = []
        bar = tqdm(dataloader['train'])
        for batch in bar:
            X, y = batch
            X, y = X.to(device), y.to(device)
            optimizer.zero_grad()
            y_hat = model(X)
            loss = criterion(y_hat, y)
            loss.backward()
            optimizer.step()
            train_loss.append(loss.item())
            bar.set_description(f"loss {np.mean(train_loss):.5f}")
        bar = tqdm(dataloader['val'])
        val_loss = []
        with torch.no_grad():
            for batch in bar:
                X, y = batch
                X, y = X.to(device), y.to(device)
                y_hat = model(X)
                loss = criterion(y_hat, y)
                val_loss.append(loss.item())
                bar.set_description(f"val_loss {np.mean(val_loss):.5f}")
        print(f"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f}")

def predict(model, X):
    model.eval()
    with torch.no_grad():
        X = torch.tensor(X).to(device)
        pred = model(X.unsqueeze(0))
        return pred

model = CharRNN(input_size=tokenizer.n_characters)
fit(model, dataloader, epochs=20)

X_new = "Era un mar en sosiego lo que se tendía a mis ojos."
X_new_encoded = tokenizer.text_to_seq(X_new)
y_pred = predict(model, X_new_encoded)
y_pred = torch.argmax(y_pred, axis=1)[0].item()
tokenizer.seq_to_text([y_pred])

for i in range(100):
  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])
  y_pred = predict(model, X_new_encoded)
  y_pred = torch.argmax(y_pred, axis=1)[0].item()
  X_new += tokenizer.seq_to_text([y_pred])

X_new

temp=1 #[1.2, 1.5, 2.0, ]
for i in range(1000):
  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])
  y_pred = predict(model, X_new_encoded)
  y_pred = y_pred.view(-1).div(temp).exp()
  top_i = torch.multinomial(y_pred, 1)[0]
  predicted_char = tokenizer.all_characters[top_i]
  X_new += predicted_char

print(X_new)

temp=1.2 #[ 1.5, 2.0, ]
for i in range(1000):
  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])
  y_pred = predict(model, X_new_encoded)
  y_pred = y_pred.view(-1).div(temp).exp()
  top_i = torch.multinomial(y_pred, 1)[0]
  predicted_char = tokenizer.all_characters[top_i]
  X_new += predicted_char

print(X_new)

temp=2.0
for i in range(1000):
  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])
  y_pred = predict(model, X_new_encoded)
  y_pred = y_pred.view(-1).div(temp).exp()
  top_i = torch.multinomial(y_pred, 1)[0]
  predicted_char = tokenizer.all_characters[top_i]
  X_new += predicted_char

print(X_new)

"""# Para Guardar el modelo"""

import torch

model_path = 'model_poemas.pth'
torch.save(model.state_dict(), model_path)

"""# Para cargar el modelo


"""

# Crear una instancia del modelo
loaded_model = CharRNN(input_size=tokenizer.n_characters)

# Cargar los parámetros guardados en el modelo
loaded_model.load_state_dict(torch.load('model_poemas.pth'))

# Cambiar al modo de evaluación (si es necesario)
loaded_model.eval()

"""Aquí hay algunos ejemplos específicos de cómo las embeddings se pueden utilizar para la generación de texto en la vida diaria:

*Un chatbot que utiliza embeddings para generar respuestas a preguntas de los usuarios.
*Una aplicación de traducción que utiliza embeddings para traducir texto de un idioma a otro.
*Un sistema de resumen de noticias que utiliza embeddings para generar resúmenes de artículos de noticias.
*Un asistente de escritura que utiliza embeddings para generar ideas para nuevos contenidos.
*Las embeddings siguen siendo una tecnología relativamente nueva, pero tienen el potencial de revolucionar la forma en que interactuamos con el lenguaje en la vida diaria.

Creación de contenido: Las embeddings se pueden utilizar para generar texto creativo, como poemas, historias, guiones, etc. Por ejemplo, un modelo de lenguaje entrenado en un conjunto de datos de poemas podría generar nuevos poemas que sean similares a los del conjunto de datos de entrenamiento.
Traducciones: Las embeddings se pueden utilizar para traducir texto de un idioma a otro. Por ejemplo, un modelo de lenguaje entrenado en un conjunto de datos de texto en inglés y español podría traducir texto del inglés al español.
Respuestas a preguntas: Las embeddings se pueden utilizar para generar respuestas a preguntas. Por ejemplo, un modelo de lenguaje entrenado en un conjunto de datos de preguntas y respuestas podría responder a preguntas sobre una variedad de temas.
Resúmenes: Las embeddings se pueden utilizar para generar resúmenes de texto. Por ejemplo, un modelo de lenguaje entrenado en un conjunto de datos de artículos de noticias podría generar resúmenes de los artículos de noticias.
"""